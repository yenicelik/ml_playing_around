{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Well, well.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependancies and supporting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a helper Class TextLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Has functions:\n",
    "    * init: to get text file into memory\n",
    "    * continue_from_model: to continue from a saved text file\n",
    "    * next_batch_pointer: return next batches\n",
    "    * reset_batch_pointer: to start over to the beginning of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TextLoader():\n",
    "    def __init__(self, batch_size, seq_length, input_file, checkpoint_filename, tensor_file = None):\n",
    "        \"\"\" Load data to generate text from. Set batch pointer to zero.\"\"\"\n",
    "        self.ip = 0 #'insruction' pointer\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        self.input_file = input_file\n",
    "        self.checkpoint_filename = checkpoint_filename #where we save this object\n",
    "        \n",
    "        #now i don't fully understand the difference between the tensorfile and the checkpoint file\n",
    "        \n",
    "        with open(self.input_file, 'r') as text_source:\n",
    "            self.chars = list(set(text_source))\n",
    "            self.vocab_size = len(self.chars)\n",
    "            \n",
    "            self.data_size = len(text_source)\n",
    "            #must define functions to convert character to one-hot vector!\n",
    "        \n",
    "        if(tensor_file):\n",
    "            #load-in old TextLoader() object\n",
    "            with open(tensor_file, 'r') as f:\n",
    "                self.tensor = np.load(tensor_file)\n",
    "        else:\n",
    "            #I guess create this tensor_file?\n",
    "            self.tensor = np.array(list(map(self.tensor_file)))\n",
    "            #create new object that loads into a savable file\n",
    "        np.save(tensor_file + '_' + str(self.pointer), self.tensor)\n",
    "    \n",
    "    def next_batch_pointer(self):\n",
    "        \"\"\" Get next batch of data. This should be a multiple of a sequence \n",
    "        length, and a multiple of the batch size \"\"\"\n",
    "        batch_size = self.batch_size * self.seq_length\n",
    "        self.num_batches = int(self.tensor.size / (batch_size))\n",
    "        if (self.num_batches == 0):\n",
    "            assert False, \"Not enough data to receive batch! :: In function next_batch_pointer of class TextLoader()\"\n",
    "        #load batch\n",
    "        self.x_batch = self.tensor[\n",
    "                        self.pointer * batch_size : \n",
    "                        (self.pointer + 1) * batch_size\n",
    "                        ]\n",
    "        self.y_batch = np.zeros(x_batch.shape)\n",
    "        self.y_batch[:-1] = np.copy(x_batch[1:0])\n",
    "        self.y_batch[-1] = np.copy(x_batch[0]) #the usual 'default' recurrent setup (predict/copy input to output)\n",
    "        \n",
    "        self.batch_pointer += 1\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    def reset_batch_pointer(self):\n",
    "            self.ip = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, params):\n",
    "        \"\"\" Get parameters and build model \"\"\"\n",
    "        #copy the parameters for object saving\n",
    "        self.params = params\n",
    "        \n",
    "        #Define what type of cell we're gonna use\n",
    "        self.cell = rnn_cell.MultiRNNCell([rnn_cell.BasicLSTM()] * params.num_layers)\n",
    "        \n",
    "        #Data input and output\n",
    "        self.input_data = tf.placeholder(tf.int32, [params.batch_size, params.seq_length])\n",
    "        self.output_data = tf.placeholder(tf.int32, [params.batch_size, params.seq_length])\n",
    "        \n",
    "        #Define the actual model\n",
    "        self.initial_state = self.cell.zero_state(params.batch_size, tf.float32)\n",
    "        \n",
    "        #define a function to define what we do during\n",
    "        with tf.variable_scope('lstm'):\n",
    "            #define a namespace, bcs we need to plug one into the seq2seq function\n",
    "            inp_W = tf.get_variable(\"softmax_W\", [params.rnn_size, params.vocab_size])\n",
    "            inp_b = tf.get_variable(\"softmax_b\", [params.rnn_size, params.vocab_size])\n",
    "        \n",
    "        \n",
    "        #what specifically does the loop function do differently thatn the dafult function?\n",
    "        #it may be that the loop function actually stops gradients... but not sure if this is actually the case bcs is stops the gradients for argmax(prev, 1)\n",
    "        def loop_fn(prev, _):\n",
    "            prev = tf.matmul(prev, inp_W) + inp_b # sample run\n",
    "            prev_symbol = tf.stop_gradient(tf.argmax(prev, 1))\n",
    "            #must return a fckn embeddeding lookup of a fucking symbol\n",
    "            \n",
    "        #rest operations (except from recurrent layer)    \n",
    "        outputs, self.final_state = seq2seq.rnn_decoder(inputs, self.initial_state, self.cell, loop_function=loop_fn, scope='lstm') #do i need to provide a loop function?? documentation says i dont need to\n",
    "    \n",
    "        self.scores = tf.matmul(output, inp_W) + inp_b\n",
    "        self.probs = tf.nn.softmax(self.scores) #calculate probabilites (softmax)\n",
    "        \n",
    "        #calculate the loss\n",
    "        loss = seq2seq.sequence_loss_by_example([self.scores],\n",
    "                                               [self.output_data],\n",
    "                                               [tf.ones([params.batch_size * params.seq_length])],\n",
    "                                               params.vocabl_size\n",
    "                                               )\n",
    "        self.cost = tf.reduce_sum(loss) / (params.batch_size*params.seq_length)\n",
    "        \n",
    "        #defining the optimization process\n",
    "        self.lr = tf.Variable(0.0, trainable=False) #wtf, you can train this?!\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        \n",
    "        #on how to update the gradients\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tf.trainable_variables()), params.grad_clip)\n",
    "        self.train_op = optimizer.apply_gradients(zip(grad, tf.trainable_variables()))\n",
    "        \n",
    "        \n",
    "        \n",
    "        #define here how the characters are converted into one-hot characters\n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\" Generate text from the trained weights etc \"\"\"\n",
    "    #model should be able to be saved\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TrainWrapper():\n",
    "    def __init__(self):\n",
    "        \"\"\" Get parameters to train with (typ of algorithm etc)\"\"\"\n",
    "    def train_model(self):\n",
    "        \"\"\" Train the model \"\"\"\n",
    "        data_load = TextLoader()\n",
    "        #implement a cPickle file loader\n",
    "        model = Model()\n",
    "        \"\"\" Train model based on input and output \"\"\"  \n",
    "        with tf.Session() as sess:\n",
    "            tf.initialize_all_variables().run()\n",
    "            saver = tf.train.Saver(tf.all_variables())\n",
    "              \n",
    "            for e in xrange(params.max_epochs):\n",
    "                sess.run(tf.assign(self.lr, self.lr * (self.decay_rate ** e)))\n",
    "                data_loader.reset_batch_pointer() # one epoch means one pass through the data\n",
    "                state = model.initial_state.eval()\n",
    "                    for b in xrange(data_load.num_batches):\n",
    "                    start = time.time() #just checking how much time it takes\n",
    "                      \n",
    "                    X_tr, y_tr = data_loader.next_batch()\n",
    "                      \n",
    "                    feed = {model.input_data: X_tr,\n",
    "                              model.output_data: y_tr,\n",
    "                              model.initial_state: state\n",
    "                           } #is state defined yet?\n",
    "                      \n",
    "                    train_loss, state, _ = sess.run([model.cost, model.final_state, model.train_op], feed)\n",
    "          \n",
    "                    end = time.time()\n",
    "              \n",
    "                    print(\"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\" .format(e * data_loader.num_batches + b,\n",
    "                    #remember to save model every 'many' iterations                                                                       args.num_epochs * data_loader.num_batches,\n",
    "                                                                                              e, train_loss, end - start))\n",
    "        \n",
    "    def char_to_one_hot(self, char):\n",
    "        \"\"\"Take a character and turn it into a one-how vector\"\"\"\n",
    "        #turn array into numpy object\n",
    "        out = np.zeros((32, 1))\n",
    "        out[np.arange(len(out)), ord(char) - '0'] = 1\n",
    "        \n",
    "    def one_hot_to_char(self, num):\n",
    "        out = np.zeros((32, 1))\n",
    "        out[np.arange(len(out)), ord(char) - '0'] = 1\n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\" Sample from the model \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Parameters():\n",
    "    def __init__(self):\n",
    "        \"\"\"Have all parameters stored here \"\"\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Actually create the classes and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-8-f35df575a66b>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-f35df575a66b>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    for b in xrange(data_load.num_batches):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "def train(params):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
